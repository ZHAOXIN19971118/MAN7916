{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Text Preprocessing and Dictionary-Based Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing is the starting point of most text analyses. In many text analysis workflows, there is a built-in preprocessing step that is used to clean and prepare the text data for analysis. However, it is still a good idea to understand what is going on under the hood. In this tutorial, we will cover the several text preprocessing steps to prepare text data for analysis. We will then use a dictionary-based approach to analyze the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Text Preprocessing Step-By-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Segmentation\n",
    "\n",
    "The first step in text preprocessing is to segment the text into sentences. This is important because many text analysis techniques operate at the sentence level. In Python, we can use the `nltk` library to segment text into sentences. NLTK is a powerful library for natural language processing that provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet. However, it is very low-level and requires a lot of code to perform simple tasks compared to some other packages we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"\"\"In this retrospective article, we outline the rationale for starting Strategic Entrepreneurship Journal. We provide evidence on the percentage of published papers in SEJ in each of 10 key themes in strategic entrepreneurship identified when the journal was founded. Evidence on progress toward goal achievement in terms of trends in submissions, desk reject and acceptances rates, and downloads, plus examples of highly cited papers and entry into key indicators such as the Financial Times list of 50 journals. We outline developments in strategic entrepreneurship and their implications for future research, notably the need to consider multiple levels of analysis and the role of context variety. Finally, we discuss some of the lessons we learned from SEJ in terms of general challenges that arise in starting a new journal. Copyright © 2017 Strategic Management Society.\"\"\"\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "for idx, sent in enumerate(sentences, 1):\n",
    "    print(f\"Sentence {idx}: {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "The next step in text preprocessing is to tokenize the text. Tokenization is the process of splitting the text into individual tokens. In Python, we can use the `nltk` library to tokenize text as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "tokens = []\n",
    "for idx, sent in enumerate(sentences, 1):\n",
    "    tokens.append(nltk.word_tokenize(sent))\n",
    "    print(f\"Sentence {idx}: \", end=\"\")\n",
    "    pprint(tokens[idx-1], compact=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing\n",
    "\n",
    "A common preprocessing step is to convert all the text to lowercase. NLTK isn't strictly necessary for this step, we generally use the `lower()` method of the string object from base Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_tokens = []\n",
    "for idx, sent in enumerate(tokens, 1):\n",
    "    lower_tokens.append([word.lower() for word in sent])\n",
    "\n",
    "    print(f\"Sentence {idx}: \", end=\"\")\n",
    "    pprint(lower_tokens[idx-1], compact=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation\n",
    "\n",
    "Another common preprocessing step is to remove punctuation from the text. Like with lowercasing, NLTK isn't strictly necessary for this step, we can use the list of common punctuation characters built into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "no_punctuation = []\n",
    "for idx, sent in enumerate(lower_tokens, 1):\n",
    "    no_punctuation.append([token for token in sent if token not in string.punctuation])\n",
    "\n",
    "    print(f\"Sentence {idx}: \", end=\"\")\n",
    "    pprint(no_punctuation[idx-1], compact=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "\n",
    "Stopwords are common words that are often removed from text data because they do not carry much information. NLTK provides a list of stopwords for many languages that we can use to remove stopwords from text data.\n",
    "\n",
    "Let's first take a look at these words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(list_of_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stopwords = []\n",
    "\n",
    "for idx, sent in enumerate(no_punctuation, 1):\n",
    "    no_stopwords.append([word for word in sent if word not in nltk.corpus.stopwords.words('english')])\n",
    "\n",
    "    print(f\"Sentence {idx}: \", end=\"\")\n",
    "    pprint(no_stopwords[idx-1], compact=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Numbers\n",
    "\n",
    "Numbers are often removed from text data or replaced with their textual representation. NLTK isn't strictly necessary for this step, we can use the `isnumerical()` method of the string object from base Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_numbers = []\n",
    "\n",
    "for idx, sent in enumerate(no_stopwords, 1):\n",
    "    no_numbers.append([word for word in sent if not word.isnumeric()])\n",
    "\n",
    "    print(f\"Sentence {idx}: \", end=\"\")\n",
    "    pprint(no_numbers[idx-1], compact=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing special characters/symbols\n",
    "\n",
    "This step can be a bit of a challenge, because it is not always clear which special characters or symbols should be removed or whether characters or symbols that are inside of words (e.g., co-opt) should be treated differently than stand-alone characters. Here, we will remove all non-alphabet symbols that are not part of a longer word (e.g., \"©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_symbols = []\n",
    "\n",
    "for idx, sent in enumerate(no_numbers, 1):\n",
    "    no_symbols.append([token for token in sent if len(token)>1 or token.isalpha()])\n",
    "\n",
    "    print(f\"Sentence {idx}: \", end=\"\")\n",
    "    pprint(no_symbols[idx-1], compact=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming/Lemmatization\n",
    "\n",
    "We won't always use stemming/lemmatization in text analysis. However, when you do, NLTK provides several tools to help you accomplish this. We'll start with stemming, which tries to shorten the word to its stem, regardless of whether the stem is itself a word. The Porter Stemmer is a commonly-used stemmer and is what we'll apply here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stems = []\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for idx, sent in enumerate(no_symbols, 1):\n",
    "    stems.append([stemmer.stem(token) for token in sent])\n",
    "\n",
    "    print(f\"Sentence {idx}: \", end=\"\")\n",
    "    pprint(stems[idx-1], compact=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try lemmatization with the WordNet Lemmatizer from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = []\n",
    "\n",
    "for idx, sent in enumerate(no_symbols, 1):\n",
    "    lemmas.append([lemmatizer.lemmatize(token) for token in sent])\n",
    "\n",
    "    print(f\"Sentence {idx}: \", end=\"\")\n",
    "    pprint(lemmas[idx-1], compact=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are, of course, other preprocessing steps that can be done such as emoji replacement, however, the ones presented here are the most commonly used ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Preprocessing using a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several Python packages have sophisticated Natural Language Processing models that will take care of most of these steps for you and often do a better job than the above tools. `SpaCy` and `Stanza` are a couple of them that I commonly use. Let's try using SpaCy to accomplish most of the above in fewer steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "spacy_results = []\n",
    "doc = nlp(text)\n",
    "\n",
    "for sent in doc.sents:\n",
    "    filtered_tokens = [\n",
    "        token.lemma_\n",
    "        for token in sent\n",
    "        if not (token.is_punct or token.is_stop or token.is_digit or token.is_space)\n",
    "    ]\n",
    "    spacy_results.append(filtered_tokens)\n",
    "\n",
    "pprint(spacy_results, compact=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to notice here:\n",
    "\n",
    "- Proper nouns are not lowercased - we can force this if we want by adding `.lower()` to `token.lemma_` in the list comprehension\n",
    "- The © symbol is still there - this is because Spacy identified this as a stand-in for the word \"copyright\" and so classified it as a noun. We could remove this by adding a check in the list comprehension to remove any tokens that are not alphabetical characters like we did before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Dictionary-Based Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary-based computer-aided text analysis is fundamentally a word-counting technique. It involves counting the number of times words from a predefined dictionary appear in a text. This technique is often used to analyze the sentiment of a text, but it can also be used to analyze other aspects of text data.\n",
    "\n",
    "Let's start by getting a sense for the frequency of words in our text data. We can use the `Counter` class from the `collections` module to count the frequency of words in our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_freq = Counter()\n",
    "for sent in no_symbols:\n",
    "    word_freq.update(sent)\n",
    "pprint(word_freq, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequently used words are very frequently used with a steep drop-off in frequency. This is a common pattern in text data and is known as Zipf's Law. Zipf's Law states that the frequency of a word is inversely proportional to its rank in the frequency table. In other words, the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, and so on. This doesn't exactly hold in our data because of the small sample size, but it is a common pattern in text data.\n",
    "\n",
    "Let's view it in a more visually appealing format using a line chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sorted_word_freq = word_freq.most_common()\n",
    "words, frequencies = zip(*sorted_word_freq)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(words, frequencies, marker='o')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequencies')\n",
    "plt.title('Word Frequencies')\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use these frequencies to find the frequencies of a 'dictionary' of words that we are interested in. For this example, you will choose what words you want to include in your dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your dictionary of words here:\n",
    "\n",
    "my_dict = ['strategic', 'entrepreneurship', 'journal', 'evidence', 'progress']\n",
    "running_total = 0\n",
    "for word in my_dict:\n",
    "    print(f\"{word}: {word_freq[word]}\")\n",
    "    running_total += word_freq[word]\n",
    "\n",
    "print(f\"\\nTotal: {running_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are the counts of the words in the dictionary, but this is a raw count and is likely to be skewed by texts with a lot of words. So we often normalize these counts by the total number of words in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = 0\n",
    "for sent in no_symbols:\n",
    "    total_words += len(sent)\n",
    "\n",
    "print(f\"\\nTotal words: {total_words}\")\n",
    "print(f\"Normalized frequency: {running_total/total_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the normalized counts of the words in the dictionary using the total number of words in the text post stopword removal. This is one viable approach, but it is common to normalize by the total number of words in the text with stopwords included in the denominator. They will give you slightly different results, but the interpretation is the similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
